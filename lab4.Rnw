\documentclass{article}

\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\setlength{\abovecaptionskip}{-5pt}
\setlength{\belowcaptionskip}{-5pt}
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{url}

\begin{document}

\title{Lab 4 - Cloud detection - Stat 215A, Fall 2017}

\author{Olivia Angiuli, Miyabi Ishihara, Yizhou Zhao}

\maketitle

\section{Introduction}

In 1999, NASA's Multiangle Imaging SpectroRadiometer (MISR) data generated a massive data set of Arctic images that help provide insight into recent global changes of ice coverage due to increasing surface air temperatures.  The dataset is rich, generated by a satellite that collected images along 233 different geographical paths, with 180 blocks in each path, providing images of any given region every 16 days.

One of the biggest statistical challenges of this dataset is to conclude, for each image, which regions are ice-covered, as opposed to snow-covered or simply unknown.  Using only a small subset (three images) of this data, each of which produced radiances from 5 different angles, we use expert labels in order to train and evaluate classification models that can help differentiate between icy versus cloudy surfaces.

<<setup, echo = FALSE, message=FALSE, warning=FALSE, fig.height = 5, fig.width = 5, fig.align='center', fig.pos='h', fig.cap="Histogram of the correlation similarity measure for different values of k">>=

library(tidyverse)
library(gridExtra)

source("R/explore.R")

# Get the data for three images
path <- "data"
image1 <- read.table(paste0('data/', 'image1.txt'), header = F)
image2 <- read.table(paste0('data/', 'image2.txt'), header = F)
image3 <- read.table(paste0('data/', 'image3.txt'), header = F)

# Add informative column names.
collabs <- c('y','x','label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
names(image1) <- collabs
names(image2) <- collabs
names(image3) <- collabs
@

\section{Exploratory Data Anaysis}

\subsection{Data background}

We are provided with three images, each of which provide the following information:

\begin{itemize}
\item \textbf{Expert labelling. } Experts visually inspected each image, classifying each pixel as cloudy (+1), not cloudy (-1), or unlabelled (0).  These are the labels with which we will train our supervised algorithms, and with which we will test our results.
\item \textbf{Radiances from five angles. } We were given satellite images for three different 275m x 275m regions, each from five different angles as shown below (where ``F" indicates the forward direction of flight and ``A" represents the rear direction):

\begin{figure}
\begin{center}
\includegraphics[width=0.35\textwidth]{extra/reference_angles}
\end{center}
\caption{The five angles from which we have radiance measures, for each of the three images.}
\end{figure}

As will be described in the next section, differences in radiance measurements of the same image from different angles can tell us about the presence or absence of clouds and/or ice.
\item \textbf{Feature values. } Shi et. al. developed three features that capture spatial information that helps distinguish between ice and clouds:
\begin{itemize}
\item \textbf{CORR - }\emph{an average linear correlation of radiation measurements at different view angles}.\\
A cloud may obstruct the view of underlying ice from some angles but not others.  Figure 1, above, illustrates a scenario in which DF provides an unobstructed view of the underlying image that the other angles cannot provide.\\
This suggests that low CORR images often suggest clouds: however, low-altitude clouds or smooth cloud-free areas may break this trend, requiring the following two features.
\item \textbf{SD - }\emph{standard deviation within groups of MISR}.\\
SD helps identify smooth surfaces (areas with low standard deviations), as well as to identify a baseline for background measurement error that can be filtered out in the model.
\item \textbf{NDAI - }\emph{a proxy for surface roughness, measured by thenormalized difference between measurements in the forward versus backward pointing cameras}.\\
The intuition is that a low-altitude cloud has more roughness than an ice- or snow-covered surface.  This feature combines with CORR to help differentiate between low CORR images that have low-altitude clouds versus are cloud-free.
\end{itemize}
\end{itemize}

\subsection{Data visualization}

The following plots visualize the three provided images.  The first set of images shows the raw data from the perspective of the AN-camera.  The second set of images shows the same three images, classified by whether an expert determined that a given area was ice (pink), unknown (green), or cloudy (blue).

<<plot-raw-data, cached=TRUE, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Raw (from AN-camera) image data for the three provided images.">>=

# Raw images
grid_arrange_shared_legend(plotRawImage(image1, "AN") +
    ggtitle("Raw satellite data of snow/ice,from AN-camera"),
    plotRawImage(image2, "AN") + ggtitle(""),
    plotRawImage(image3, "AN") + ggtitle(""),
    ncol=3, nrow=1, position="right")
@

<<plot-expert-labelled-data, cached=TRUE, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Expert-labelled image data for the three provided images.">>=

# Expert-labelled images
grid_arrange_shared_legend(plotClasses(image1) +
                             ggtitle("Expert-labelled satellite data of snow/ice"),
                           plotClasses(image2) + ggtitle(""),
                           plotClasses(image3) + ggtitle(""),
                           ncol=3, nrow=1, position="right")
@

\subsection{Do different angles provide different information?}

Yes!  Below, we show image 1 from all 5 different angles that we are provided in the data set: from left to right, 70.5$^{\circ}$ (DF), 60.0$^{\circ}$ (CF), 45.6$^{\circ}$ (BF), 26.1$^{\circ}$ (AF) in the forward directions,  and 0$^{\circ}$ (AN).

One can spot higher resolution in the bottom left corner (which corresponds to mostly cloudy and/or unknown regions) as the angles becoming increasingly acute.  This supports the reasoning for Shi et. al.'s construction of the ``CORR" and ``NDAI" features: cloudy regions might have more variation in radiances between different angles.

<<plot-images-by-angle, cached=TRUE, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Image 1, by increasingly acute angles.">>=

grid_arrange_shared_legend(plotRawImage(image1, "DF") +
    ggtitle("Image 1, taken from increasingly acute angles"),
    plotRawImage(image1, "CF") + no_axis + ggtitle(""),
    plotRawImage(image1, "BF") + no_axis + ggtitle(""),
    plotRawImage(image1, "AF") + no_axis + ggtitle(""),
    plotRawImage(image1, "AN") + no_axis + ggtitle(""),
    ncol=5, nrow=1, position="right")
@

Plotting the conditional densities of cloud- versus ice-covered regions confirms that different angles do indeed help differentiate between cloud and ice.  We notice two trends in particular.  First, icy regions tend to occur at a resolution of around 275.  This can effectively set a high ``prior" for iciness for pixels with a similar resolution.  Second, we notice that cloudy regions have a much wider range of resolutions that change with the angle of measurement.  Again, this confirms Shi et. al.'s intuition that differences between measurements from different angles often correspond to cloudiness.

<<plot-densities-by-angle, cached=TRUE, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Conditional densities of ice vs. cloud vs. unknown by different angles. Cloud resolutions become increasingly bimodal as measurement angle decreases.">>=

grid_arrange_shared_legend(plot_conditional_densities(image1, "DF") +
    ggtitle("Image 1 conditional densities, by measurement angles") +
    no_axis + xlab("DF"),
    plot_conditional_densities(image1, "CF") + no_axis + ggtitle(""),
    plot_conditional_densities(image1, "BF") + no_axis + ggtitle(""),
    plot_conditional_densities(image1, "AF") + no_axis + ggtitle(""),
    plot_conditional_densities(image1, "AN") + no_axis + ggtitle(""),
    ncol=5, nrow=1, position="right")

@

\subsection{How do features help distinguish between cloud and ice?}

Recall that the three features, as described in detail above, are:
\begin{itemize}
\item CORR (correlation of images from different view angles)
\item SD (the standard deviation within groups of MISR)
\item NDAI (a proxy for surface roughness, measured by the normalized difference between measurements in the forward vs. backward pointing cameras)
\end{itemize}

By itself, NDAI has the most discriminatory power between the presence of ice and the absence of ice (either cloudy or unknown).  This can be seen by the distinct peaks in the leftmost graph below.

<<plot-features-for-image-1, cached=TRUE, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Conditional densities of ice vs. cloud vs. unknown by different angles. Cloud resolutions become increasingly bimodal as measurement angle decreases.">>=

grid_arrange_shared_legend(plot_conditional_densities(image1, "NDAI") +
    ggtitle("Image 1 NDAI"),
    plot_conditional_densities(image1, "CORR") + ggtitle("Image 1 CORR"),
    plot_conditional_densities(image1, "SD") + ggtitle("Image 1 SD"),
    ncol=3, nrow=1, position="right")
@

The discriminatory power of NDAI can also be seen from the following side-by-side plot of NDAI resolution values versus the true classes of image 1.  Note that higher values of NDAI tend to correspond to ``no ice" regions.

<<plot-ndai-values, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="High NDAI values tend to correspond to unknown or cloudy regions; low NDAI values tend to correspond to icy regions.">>=

grid.arrange(plotRawImage(image1, "NDAI") + ggtitle("Image 1 NDAI"),
    plotClasses(image1) + ggtitle("True classes"), ncol=2, nrow=1)

@

\newpage

Shi et. al. also discuss how NDAI and CORR, together, provide even more information than the two features alone: they hypothesize that high CORR + low NDAI images would more strongly suggest ice than high CORR alone.  This relationship, though, is not readily seen in the following pairwise feature plots, largely because there are few high CORR pixels that correspond to ice-covered regions.

<<ndai-vs-corr, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="High NDAI values tend to correspond to unknown or cloudy regions; low NDAI values tend to correspond to icy regions.">>=

# plot all pairwise features, across all three image datasets
grid_arrange_shared_legend(
  plot_feature_vs_feature("NDAI", "CORR") + 
    ggtitle("Pairwise feature scatterplots"),
  plot_feature_vs_feature("NDAI", "SD") + ggtitle(""),
  plot_feature_vs_feature("CORR", "SD") + ggtitle(""))

@

\section{Classifying ice- versus snow-covered surfaces}

The classification approach of Shi et. al. hinges upon an enhanced linear correlation matching (ELCM) algorithm.  However, we noted that this approach depended on having data images available from multiple orbits and/or consecutive MISR blocks.  Since we do not have access to this full data set, reproducing this ELCM approach is not possible.

For that reason, we instead resort to more classic classification methods: QDA/LDA, logistic regression, and random forests.  First, we train baseline versions of these methods.   We note that random forests produces the best AUC values of the three baseline models, so we proceed to do additional parameter tuning and analysis for that model.

We decided before training these methods on a few approaches:

\begin{itemize}
\item \textbf{Three-fold cross validation. } Since labels of adjacent pixels in an image are not truly independent, we decided to perform three-fold cross validation: that is, we would train three different versions of each model, each time setting aside an entire image for testing purposes.\\
This is much preferred over randomly sampling points, since regions of ice and cloud often span many adjacent pixels, meaning that training on one of these pixels would enhance the test performance of the pixels nearby to it.
\item \textbf{Training only on positive- and negative- expert labels. } We do not train our model on data points whose expert classification was a 0 (unknown): we would only like our model to learn classification based on points whose true value is known.
\end{itemize}

\section{LDA/QDA}

<<fit-LDA-QDA-part, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2.5, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="ROC curve for LDA/QDA three-fold cross-validation.">>=

source("R/explore.R")

library(dplyr)
library(reshape2)
library(gridExtra)
library(MASS)
library(ROCR)
library(ggplot2)

path <- "data"
image1 <- read.table(paste0('data/', 'image1.txt'), header = F)
image2 <- read.table(paste0('data/', 'image2.txt'), header = F)
image3 <- read.table(paste0('data/', 'image3.txt'), header = F)

# Add informative column names.
collabs <- c('y','x','label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
names(image1) <- collabs
names(image2) <- collabs
names(image3) <- collabs

training.image <- list(image1,image2,image3)

#Only to train and testing on the labeled data
Qimage1 = mutate(image1, Image = 1) %>% filter(label != 0)
Qimage2 = mutate(image2, Image = 2) %>% filter(label != 0)
Qimage3 = mutate(image3, Image = 3) %>% filter(label != 0)
images = rbind(Qimage1, Qimage2, Qimage3)


#QDA with one-image-left-out CV
qda1 = qda(label ~ NDAI + SD + CORR, data = images %>% filter(Image != 1))
qda2 = qda(label ~ NDAI + SD + CORR, data = images %>% filter(Image != 2))
qda3 = qda(label ~ NDAI + SD + CORR, data = images %>% filter(Image != 3))

#predict the left-out image with QDA
qda1.pred = predict(qda1, type="prob",newdata=Qimage1)
qda2.pred = predict(qda2, type="prob",newdata=Qimage2)
qda3.pred = predict(qda3, type="prob",newdata=Qimage3)

#ROC values and ROC curves
qda.roc1 = performance(prediction(qda1.pred$posterior[,2], Qimage1$label), "tpr", "fpr")
qda.roc2 = performance(prediction(qda2.pred$posterior[,2], Qimage2$label), "tpr", "fpr")
qda.roc3 = performance(prediction(qda3.pred$posterior[,2], Qimage3$label), "tpr", "fpr")

#AUC scores
qda.auc1 = performance(prediction(qda1.pred$posterior[,2], Qimage1$label), "auc")@y.values[[1]]
qda.auc2 = performance(prediction(qda2.pred$posterior[,2], Qimage2$label), "auc")@y.values[[1]]
qda.auc3 = performance(prediction(qda3.pred$posterior[,2], Qimage3$label), "auc")@y.values[[1]]

#Plot the ROC curve
qda.cv = rbind(data.frame(TPR = qda.roc1@y.values[[1]], FPR = qda.roc1@x.values[[1]], fold = factor(1)),
               data.frame(TPR = qda.roc2@y.values[[1]], FPR = qda.roc2@x.values[[1]], fold = factor(2)),
               data.frame(TPR = qda.roc3@y.values[[1]], FPR = qda.roc3@x.values[[1]], fold = factor(3)))

qda.roc.info = ggplot(qda.cv) +
  geom_line(aes(x=FPR,y=TPR, group=fold, color=fold)) +
  scale_colour_discrete(name  ="Image Left Out") +
  ggtitle("QDA")

#lda part
lda1 = lda(label ~ NDAI + SD + CORR, data = images %>% filter(Image != 1))
lda2 = lda(label ~ NDAI + SD + CORR, data = images %>% filter(Image != 2))
lda3 = lda(label ~ NDAI + SD + CORR, data = images %>% filter(Image != 3))

lda1.pred = predict(lda1, type="prob",newdata=Qimage1)
lda2.pred = predict(lda2, type="prob",newdata=Qimage2)
lda3.pred = predict(lda3, type="prob",newdata=Qimage3)

lda.roc1 = performance(prediction(lda1.pred$posterior[,2], Qimage1$label), "tpr", "fpr")
lda.roc2 = performance(prediction(lda2.pred$posterior[,2], Qimage2$label), "tpr", "fpr")
lda.roc3 = performance(prediction(lda3.pred$posterior[,2], Qimage3$label), "tpr", "fpr")

lda.auc1 = performance(prediction(lda1.pred$posterior[,2], Qimage1$label), "auc")@y.values[[1]]
lda.auc2 = performance(prediction(lda2.pred$posterior[,2], Qimage2$label), "auc")@y.values[[1]]
lda.auc3 = performance(prediction(lda3.pred$posterior[,2], Qimage3$label), "auc")@y.values[[1]]

lda.cv = rbind(data.frame(TPR = lda.roc1@y.values[[1]], FPR = lda.roc1@x.values[[1]], fold = factor(1)),
               data.frame(TPR = lda.roc2@y.values[[1]], FPR = lda.roc2@x.values[[1]], fold = factor(2)),
               data.frame(TPR = lda.roc3@y.values[[1]], FPR = lda.roc3@x.values[[1]], fold = factor(3)))

lda.roc.info = ggplot(lda.cv) +
  geom_line(aes(x=FPR,y=TPR, group=fold, color=fold)) +
  scale_colour_discrete(name  ="Image Left Out") +
  ggtitle("LDA")

grid_arrange_shared_legend(lda.roc.info,qda.roc.info,ncol =2 ,nrow = 1, position = "right")
@

Quadradic discriminant analysis(LDA) for classification aims to separate the feature space using lines(hyperplanes in higher dimenstion). It assumes that  each class $i$ has a Gaussian distribution with mean $\mu_i$ and covariance matrix $\Sigma_i$, that is 

$$
p(y = i | x) = \frac{\pi_i|2\pi\Sigma_i|^{-\frac{1}{2}}\exp[-\frac{1}{2}(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i)]}{\sum_k\pi_k|2\pi\Sigma_k|^{-\frac{1}{2}}\exp[-\frac{1}{2}(x-\mu_k)^T\Sigma_i^{-1}(x-\mu_k)]}
$$

Linear discriminant analysis(LDA) is a special case of QDA, which makes further assumption that the covariance matrix are the same across all classes. Our problem deals with binary data, and we can classify each point by selecting a probability threhold and assign a hard hyperline to determine whether a pixel belongs to clear or cloud. 

Figure ??? shows the ROC curves for the leave-one-image-out cross validation results from LDA and QDA models, in which QDA performs a little better than LDA in the first and second fold, which is in line with our exploratory analysis: the covariances for NDAI/CORR/SD may be difference between cloud and clear pixels. 

\begin{table}
\begin{center}
  \begin{tabular}{ | c | c | c |}
    \hline
    LDA & QDA & FOLD \\ \hline
    0.951 & 0.959 & 1 \\ 
    0.954 & 0.97 & 2 \\
    0.895& 0.887 & 3\\
    \hline
  \end{tabular}
\end{center}
\end{table}

Cross-validation for QDA and LDA tells that while they work really well in the first and second fold, finding that the AUC scores almost 1, it falls to perform as well in the third fold, where we regard image1 and image2 as training data and image3 as testing data. 

<<fit-qda-3, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2.5, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Confusioin plots for qda">>=

#Find the threshold(alpha-values)
qda.thresh1 = qda.roc1@alpha.values[[1]][which.max(qda.roc1@y.values[[1]] - qda.roc1@x.values[[1]])]
qda.thresh2 = qda.roc2@alpha.values[[1]][which.max(qda.roc2@y.values[[1]] - qda.roc2@x.values[[1]])]
qda.thresh3 = qda.roc3@alpha.values[[1]][which.max(qda.roc3@y.values[[1]] - qda.roc3@x.values[[1]])]

#use the mean of the best thresholdes as our classifier
thresh = mean(c(qda.thresh1,qda.thresh2,qda.thresh3)) #0.21

#Get the confusion plots
Qimage1 = Qimage1 %>% mutate(posterior = qda1.pred$posterior[,2]) %>% 
  mutate(confusion = ifelse(label == -1,ifelse(posterior < thresh,"TN","FN"),ifelse(posterior >= thresh,"TP","FP")))
         
Qimage2 = Qimage2 %>% mutate(posterior = qda2.pred$posterior[,2]) %>% 
  mutate(confusion = ifelse(label == -1,ifelse(posterior < thresh,"TN","FN"),ifelse(posterior >= thresh,"TP","FP")))

Qimage3 = Qimage3 %>% mutate(posterior = qda3.pred$posterior[,2]) %>% 
  mutate(confusion = ifelse(label == -1,ifelse(posterior < thresh,"TN","FN"),ifelse(posterior >= thresh,"TP","FP")))

ggconf <- list(geom_point(aes(x=x, y=y, group=confusion, colour=confusion)),
               scale_colour_manual(values = c("TP" = "white","FP" = "black","TN" = "#1eee2b","FN" = "#ff9428","Unknown" = "#777777")),
               theme(legend.position="right", aspect.ratio=1))

# theme to remove axis
no_axis <- theme(axis.line=element_blank(),
                 axis.text.x=element_blank(),
                 axis.text.y=element_blank(),
                 axis.ticks=element_blank(),
                 axis.title.y=element_blank(),
                 axis.title.x=element_blank())

qda1.conf <- ggplot(Qimage1) + ggconf +
  ggtitle("Image 1") + no_axis

qda2.conf <- ggplot(Qimage2) + ggconf +
  ggtitle("Image 2") + no_axis

qda3.conf <- ggplot(Qimage3) + ggconf +
  ggtitle("Image 3") + no_axis

grid_arrange_shared_legend(qda1.conf,qda2.conf,qda3.conf,ncol =3 ,nrow = 1, position = "right")
@

Figure ??? shows that confusion plot after we selected the threshold to be $0.21$, which is the average of the thresholds in the three-fold validation to make the point $(TRR, FPR)$ farest away from the line $y = x$. As the above plot shows, the classification suffers more from false positive and 
false negetive with some regions completely misclassified. Actually, those regions are hard to be classified from our bare eyes.  Calculation shows that the false positive rates of the three folds are $0.121,0.122,0.195$ and false negetive rates are $0.018,0.001,0.165$. The third fold cross-validation 
has such a high false negative rate because image3 has a small area of cloud and some parts of it on the upper left of the image are misclassified. However, the main problem is that the qda/lda classifier is hard to judge whether the "dark" regions in the images belong to cloud to clear, for example, 
the middle lower left region in image one, the lower right region in image two and lower middle and lower right regions on image three. All of that informs us of the irregularity of image three, for its large fraction of unknown area and complicated cloud and clear regions.

\subsection{Logistic regression}

Logistic regression models the probability of the response taking a particular value based on the covariates. In this case, we model the probability of a pixel $Y_i$ being cloudy, given the three features: 

$$
\pi_i = P(Y_i = 1|X_i = x_i) = \frac{e^(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3)}{1 + e^(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3)}, 
$$

or 

$$
log(\frac{\pi_i}{1-\pi_i}) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3, 
$$

where $\beta_1, \beta_2, \beta_3$ correspond to the estimates of our three features.\\

Logistic regression does not make some of the key assumptions made by linear regression. Specifically, it does not assume linear relationship between the outcome and covariates, normality of covariates, or homoscedasticity. However, one assumption that is not satisfied in this dataset is the requirement that $Y_1, Y_2, ..., Y_n$ be independently distributed. Pixels have spatial correlation in that those that are close to each other are more likely to have the same binary outcome. \\ 

ROC plot shows that the classification accuracies for the three images vary; Prediction made on image 2 has the highest accuracy rate, whereas predictions on images 1 and 3 have lower accuracy rates, as indicated by smaller AUC. Unlike in the results obtained in LDA, QDA, and random forest, prediction on image 3 is not substantially worse than predictions made on other images. 

<<fit-logistic-regression, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2.5, fig.width = 5, fig.align='center', fig.pos='h', fig.cap="ROC curve for logistic regression three-fold cross-validation.">>=

source("R/logistic_regression.R")

# plot ROC for three images 
ggplot() +
  geom_line(data = data, aes(x = fprs, y = tprs, col = group)) + 
  scale_color_discrete(name = "Predictions on \nImage Number",
                    labels = c("1 (AUC = 0.897)", "2 (AUC = 0.953)", 
                               "3 (AUC = 0.898)")) + 
  theme_bw() + 
  xlab("false positive rate") + 
  ylab("true positive rate")

@


\subsection{Random forests}

Random forests are an ensemble learning method that combine the prediction of multiple component decision trees in order to make a probabilistic determination of the class of an input.

Random forests are ``random" in a few different steps:
\begin{itemize}
\item Tree bagging. Each component tree randomly selects, via bootstrapping, a training subset from the total training set.
\item Feature bagging. At each candidate split of a component tree, a random subset of the features are used.
\end{itemize}

This is illustrated visually in the below picture.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.5\textwidth]{extra/rf_explanation.jpg}
\end{center}
\caption{Graphical introduction to random forests [2].}
\end{figure}

In our case, we fit random forest with 100 trees three times (using the three-fold cross validation approach described above).  The resulting ROC-curve is shown below, and reveals very strong performance, shown by the model's ability to achieve a high true positive rate with quite a low corresponding false negative rate.

<<fit-random-forest, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2.5, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="ROC curve for random forest three-fold cross-validation.">>=

source("R/random_forest.R")

# generated predictions for each of the three-fold CV data sets
prediction3 = trainAndEvaluateRF(train_data_12, image3_filtered, 100)
prediction2 = trainAndEvaluateRF(train_data_13, image2_filtered, 100)
prediction1 = trainAndEvaluateRF(train_data_23, image1_filtered, 100)

# calculate false positive and true positive rates into dataframe
fpr_tpr_rates_df <- data.frame(
  rbind(calculateROCValues(prediction3, 3),
      calculateROCValues(prediction2, 2),
      calculateROCValues(prediction1, 1)))
colnames(fpr_tpr_rates_df) <- c("fpr", "tpr", "image_num")

# plot results
ggplot(fpr_tpr_rates_df) +
  geom_line(aes(x = fpr, y = tpr, color = factor(image_num))) +
  labs(x = "False positive rate",
    y = "True positive rate",
    title = "Random Forest ROC curve: Leave-one-out CV") +
  scale_color_discrete(name = "Predictions on Image Num",
                      labels = c("1 (AUC=0.956)",
                                 "2 (AUC=0.965)",
                                 "3 (AUC=0.885)")) +
  # clean up plot a little bit
  theme_bw() +
  # make the title and axes labels smaller
  theme(panel.border = element_blank(),
        legend.position = "right",
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 9),
        plot.title = element_text(size = 9),
        axis.title.x = element_text(size = 9),
        axis.title.y = element_text(size = 9),
        axis.text=element_text(size = 9))

@

\subsubsection{Parameter tuning}

The three main parameters to tune are the number of trees in the model, the maximum number of leaves (nodes) in each tree, and the number of features to use in each tree.

% Note that parameter tuning takes a long time, so we didn't want it to
% run every time the report compiled.  You can view the code in the
% parameter_tuning_rf.R file.

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{extra/parameter_tuning_graphs}
\end{center}
\end{figure}

First, we examine the marginal behavior between AUC and each parameter.  We see from the above graph that AUC is maximized with:
\begin{itemize}
\item \textbf{A high number of trees}. This is intuitive because more trees lead to a more stable prediction.  Since each tree uses different baseline data and different features, overfitting is usually not a big concern.
\item \textbf{A moderate number of maxnodes}. This suggests that having too many leaves in any given component tree may lead to overfitting.
\item \textbf{A small number of features tried in each tree.}  At first, this may be counterintuitive: you would think if more features are considered within each tree, this would lead to a strictly better result.  However, since our features may be correlated, then a small number of features may be better for prediction since it may decouple the correlation between features and avoid overfitting.
\end{itemize}

The below table shows the three best and three worst parameter combinations, in terms of AUC performance. The best parameter combination coresponds to 500 trees with a maximum of 10 nodes per tree and 1 feature tried at each point, which outputs a model with an AUC of 0.900.

\begin{center}
    \begin{tabular}{ | l | l | l | l |}
    \hline
    Num Trees & Max Nodes & Num Features & AUC \\ \hline
    500 & 10 & 1 & 0.900 \\ \hline
    100 & 10 & 1 & 0.898 \\ \hline
    50 & 10 & 1 & 0.896 \\ \hline
    ... & ... & ... & ... \\ \hline
    100 & 10 & 3 & 0.810 \\ \hline
    50 & 10 & 3 & 0.809 \\ \hline
    10 & 10 & 3 & 0.803 \\ \hline
    \hline
    \end{tabular}
\end{center}



\subsubsection{Systematic misclassification errors}

In the three images, at least 80\% of the pixels are correctly classified as cloudy or clear. Prediction made on image 2 has the highest accuracy with very low false negative rate, whereas the prediction made on image 3 has the lowest accuracy rate. 

\begin{table}[h]
\centering
%\caption{Misclassification rate}
%\label{misclassification_rate}
\begin{tabular}{l|l|l|ll}
\cline{1-4}
 & correctly classified (\%) & false positive (\%) & false negative (\%) &  \\ \cline{1-4}
image 1 & 89.0 & 4.8 & 6.2 &  \\
image 2 & 93.1 & 5.6 & 1.3 &  \\
image 3 & 80.4 & 8.6 & 11.0 &  \\
all images & 88.4 & 6.2 & 5.5 &  \\ \cline{1-4}
\end{tabular}
\end{table}

There are certain feature values at which misclassification occurs more frequently than others. For example, false negatives tend to occur at either lower or upper extreme values of NDAI. False positives occur specifically when both CORR and NDAI are not extreme values. NDAI, which differs substantially across the three images, is the most indicative feature that tells us how pixels are misclassified. 

<<misclassification_plot, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2.5, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Patterns in misclassification errors by different feature values. Grey points denote pixels that are correctly classified, blue denotes clear pixels  incorrectly classified as cloud, and pink denotes cloudy pixels incorrectly classified as clear.">>=


# for each image, create data that has both predicted and actual labels
data1 <- cbind(prediction1, image1_filtered)
data2 <- cbind(prediction2, image2_filtered)
data3 <- cbind(prediction3, image3_filtered)
data <- rbind(data1, data2, data3)

# set a threshold
threshold <- 0.5

# create a categorical variable that indicates whether a pixels is correctly classified or not correctly classified
data$class <- NA
data$class[data$predicted > threshold & data$true == 1] <- "correctly classified"
data$class[data$predicted <= threshold & data$true == 0] <- "correctly classified"
data$class[data$predicted > threshold & data$true == 0] <- "false positive"
data$class[data$predicted <= threshold & data$true == 1] <- "false negative"
# convert the variable as factor
data$class <- factor(data$class, levels = c("correctly classified", "false positive", "false negative"))


# plot NDAI vs SD with colors indicating misclassification type
g1 <- 
  ggplot() + 
  geom_point(data = data, aes(x = NDAI, y = SD, color = class), alpha = 0.1) + 
  scale_color_manual(values = c("gray70", "turquoise2", "deeppink")) +
  guides(colour = guide_legend(override.aes = list(alpha = 1))) + 
  theme_bw() + 
  theme(legend.position = "none")

# plot NDAI vs CORR 
g2 <- 
  ggplot() + 
  geom_point(data = data, aes(x = NDAI, y = CORR, color = class), alpha = 0.1) + 
  scale_color_manual(name = "", values = c("gray70", "turquoise2", "deeppink")) +
  theme_bw() + 
  theme(legend.position = "none")

# plot SD vs CORR 
g3 <- 
  ggplot() + 
  geom_point(data = data, aes(x = SD, y = CORR, color = class), alpha = 0.1) + 
  scale_color_manual(name = "", values = c("gray70", "turquoise2", "deeppink")) +
  theme_bw() + 
  theme(legend.position = "none")

grid.arrange(g1, g2, g3, ncol = 3)

@


\subsubsection{Generalizability of the model}

Ultimately, a model learns best about the subspaces that it has trained on, and a random forest model is no different.  Decision criteria within every component decision tree of a random forest are chosen to be optimal for data whose ranges are similar to those of the training examples.\\

Although we don't have information about the variability of other images within the feature space of NDAI, CORR, and SD, we can reasonably assume that \textbf{the model will perform fairly well in images whose feature combinations are similar to those seen in Images 1, 2, and 3}, and will perform less accurately for images whose feature combinations are not.

\section{Concluding thoughts}

\section{References}

(1)  Shi, Tao, et al. "Daytime arctic cloud detection based on multi-angle satellite data with case studies." Journal of the American Statistical Association 103.482 (2008): 584-593.
(2) Random Forest Interactive Discussion with Example. \url{https://www.youtube.com/watch?v=ajTc5y3OqSQ}

\end{document}
