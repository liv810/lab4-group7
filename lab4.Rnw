\documentclass{article}

\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\setlength{\abovecaptionskip}{-5pt}
\setlength{\belowcaptionskip}{-5pt}
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{url}

\begin{document}

\title{Lab 4 - Cloud detection - Stat 215A, Fall 2017}

\author{Olivia Angiuli, Miyabi Ishihara, Yizhou Zhao}

\maketitle

\section{Introduction}

In 1999, NASA's Multiangle Imaging SpectroRadiometer (MISR) data generated a massive data set of Arctic images that help provide insight into recent global changes of ice coverage due to increasing surface air temperatures.  The dataset is rich, generated by a satellite that collected images along 233 different geographical paths, with 180 blocks in each path, providing images of any given region every 16 days.

One of the biggest statistical challenges of this dataset is to conclude, for each image, which regions are ice-covered, as opposed to snow-covered or simply unknown.  Using only a small subset (three images) of this data, each of which produced radiances from 5 different angles, we use expert labels in order to train and evaluate classification models that can help differentiate between icy versus cloudy surfaces.

<<setup, echo = FALSE, message=FALSE, warning=FALSE, fig.height = 5, fig.width = 5, fig.align='center', fig.pos='h', fig.cap="Histogram of the correlation similarity measure for different values of k">>=

library(tidyverse)
library(gridExtra)

source("R/explore.R")

# Get the data for three images
path <- "data"
image1 <- read.table(paste0('data/', 'image1.txt'), header = F)
image2 <- read.table(paste0('data/', 'image2.txt'), header = F)
image3 <- read.table(paste0('data/', 'image3.txt'), header = F)

# Add informative column names.
collabs <- c('y','x','label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
names(image1) <- collabs
names(image2) <- collabs
names(image3) <- collabs
@

\section{Exploratory Data Anaysis}

\subsection{Data background}

We are provided with three images, each of which provide the following information:

\begin{itemize}
\item \textbf{Expert labelling. } Experts visually inspected each image, classifying each pixel as cloudy (+1), not cloudy (-1), or unlabelled (0).  These are the labels with which we will train our supervised algorithms, and with which we will test our results.
\item \textbf{Radiances from five angles. } We were given satellite images for three different 275m x 275m regions, each from five different angles as shown below (where ``F" indicates the forward direction of flight and ``A" represents the rear direction):

\begin{figure}
\begin{center}
\includegraphics[width=0.35\textwidth]{extra/reference_angles}
\end{center}
\caption{The five angles from which we have radiance measures, for each of the three images.}
\end{figure}

As will be described in the next section, differences in radiance measurements of the same image from different angles can tell us about the presence or absence of clouds and/or ice.
\item \textbf{Feature values. } Shi et. al. developed three features that capture spatial information that helps distinguish between ice and clouds:
\begin{itemize}
\item \textbf{CORR - }\emph{an average linear correlation of radiation measurements at different view angles}.\\
A cloud may obstruct the view of underlying ice from some angles but not others.  Figure 1, above, illustrates a scenario in which DF provides an unobstructed view of the underlying image that the other angles cannot provide.\\
This suggests that low CORR images often suggest clouds: however, low-altitude clouds or smooth cloud-free areas may break this trend, requiring the following two features.
\item \textbf{SD - }\emph{standard deviation within groups of MISR}.\\
SD helps identify smooth surfaces (areas with low standard deviations), as well as to identify a baseline for background measurement error that can be filtered out in the model.
\item \textbf{NDAI - }\emph{a proxy for surface roughness, measured by thenormalized difference between measurements in the forward versus backward pointing cameras}.\\
The intuition is that a low-altitude cloud has more roughness than an ice- or snow-covered surface.  This feature combines with CORR to help differentiate between low CORR images that have low-altitude clouds versus are cloud-free.
\end{itemize}
\end{itemize}

\subsection{Data visualization}

The following plots visualize the three provided images.  The first set of images shows the raw data from the perspective of the AN-camera.  The second set of images shows the same three images, classified by whether an expert determined that a given area was ice (pink), unknown (green), or cloudy (blue).

<<plot-raw-data, cached=TRUE, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Raw (from AN-camera) image data for the three provided images.">>=

# Raw images
grid_arrange_shared_legend(plotRawImage(image1, "AN") +
    ggtitle("Raw satellite data of snow/ice,from AN-camera"),
    plotRawImage(image2, "AN") + ggtitle(""),
    plotRawImage(image3, "AN") + ggtitle(""),
    ncol=3, nrow=1, position="right")
@

<<plot-expert-labelled-data, cached=TRUE, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Expert-labelled image data for the three provided images.">>=

# Expert-labelled images
grid_arrange_shared_legend(plotClasses(image1) +
                             ggtitle("Expert-labelled satellite data of snow/ice"),
                           plotClasses(image2) + ggtitle(""),
                           plotClasses(image3) + ggtitle(""),
                           ncol=3, nrow=1, position="right")
@

\subsection{Do different angles provide different information?}

Yes!  Below, we show image 1 from all 5 different angles that we are provided in the data set: from left to right, 70.5$^{\circ}$ (DF), 60.0$^{\circ}$ (CF), 45.6$^{\circ}$ (BF), 26.1$^{\circ}$ (AF) in the forward directions,  and 0$^{\circ}$ (AN).

One can spot higher resolution in the bottom left corner (which corresponds to mostly cloudy and/or unknown regions) as the angles becoming increasingly acute.  This supports the reasoning for Shi et. al.'s construction of the ``CORR" and ``NDAI" features: cloudy regions might have more variation in radiances between different angles.

<<plot-images-by-angle, cached=TRUE, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Image 1, by increasingly acute angles.">>=

grid_arrange_shared_legend(plotRawImage(image1, "DF") +
    ggtitle("Image 1, taken from increasingly acute angles"),
    plotRawImage(image1, "CF") + no_axis + ggtitle(""),
    plotRawImage(image1, "BF") + no_axis + ggtitle(""),
    plotRawImage(image1, "AF") + no_axis + ggtitle(""),
    plotRawImage(image1, "AN") + no_axis + ggtitle(""),
    ncol=5, nrow=1, position="right")
@

Plotting the conditional densities of cloud- versus ice-covered regions confirms that different angles do indeed help differentiate between cloud and ice.  We notice two trends in particular.  First, icy regions tend to occur at a resolution of around 275.  This can effectively set a high ``prior" for iciness for pixels with a similar resolution.  Second, we notice that cloudy regions have a much wider range of resolutions that change with the angle of measurement.  Again, this confirms Shi et. al.'s intuition that differences between measurements from different angles often correspond to cloudiness.

<<plot-densities-by-angle, cached=TRUE, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Conditional densities of ice vs. cloud vs. unknown by different angles. Cloud resolutions become increasingly bimodal as measurement angle decreases.">>=

grid_arrange_shared_legend(plot_conditional_densities(image1, "DF") +
    ggtitle("Image 1 conditional densities, by measurement angles") +
    no_axis + xlab("DF"),
    plot_conditional_densities(image1, "CF") + no_axis + ggtitle(""),
    plot_conditional_densities(image1, "BF") + no_axis + ggtitle(""),
    plot_conditional_densities(image1, "AF") + no_axis + ggtitle(""),
    plot_conditional_densities(image1, "AN") + no_axis + ggtitle(""),
    ncol=5, nrow=1, position="right")

@

\subsection{How do features help distinguish between cloud and ice?}

Recall that the three features, as described in detail above, are:
\begin{itemize}
\item CORR (correlation of images from different view angles)
\item SD (the standard deviation within groups of MISR)
\item NDAI (a proxy for surface roughness, measured by the normalized difference between measurements in the forward vs. backward pointing cameras)
\end{itemize}

By itself, NDAI has the most discriminatory power between the presence of ice and the absence of ice (either cloudy or unknown).  This can be seen by the distinct peaks in the leftmost graph below.

<<plot-features-for-image-1, cached=TRUE, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Conditional densities of ice vs. cloud vs. unknown by different angles. Cloud resolutions become increasingly bimodal as measurement angle decreases.">>=

grid_arrange_shared_legend(plot_conditional_densities(image1, "NDAI") +
    ggtitle("Image 1 NDAI"),
    plot_conditional_densities(image1, "CORR") + ggtitle("Image 1 CORR"),
    plot_conditional_densities(image1, "SD") + ggtitle("Image 1 SD"),
    ncol=3, nrow=1, position="right")
@

The discriminatory power of NDAI can also be seen from the following side-by-side plot of NDAI resolution values versus the true classes of image 1.  Note that higher values of NDAI tend to correspond to ``no ice" regions.

<<plot-ndai-values, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="High NDAI values tend to correspond to unknown or cloudy regions; low NDAI values tend to correspond to icy regions.">>=

grid.arrange(plotRawImage(image1, "NDAI") + ggtitle("Image 1 NDAI"),
    plotClasses(image1) + ggtitle("True classes"), ncol=2, nrow=1)

@

\newpage

Shi et. al. also discuss how NDAI and CORR, together, provide even more information than the two features alone: they hypothesize that high CORR + low NDAI images would more strongly suggest ice than high CORR alone.  This relationship, though, is not readily seen in the following pairwise feature plots, largely because there are few high CORR pixels that correspond to ice-covered regions.

<<ndai-vs-corr, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="High NDAI values tend to correspond to unknown or cloudy regions; low NDAI values tend to correspond to icy regions.">>=

# plot all pairwise features, across all three image datasets
grid_arrange_shared_legend(
  plot_feature_vs_feature("NDAI", "CORR") + 
    ggtitle("Pairwise feature scatterplots"),
  plot_feature_vs_feature("NDAI", "SD") + ggtitle(""),
  plot_feature_vs_feature("CORR", "SD") + ggtitle(""))

@

\section{Classifying ice- versus snow-covered surfaces}

The classification approach of Shi et. al. hinges upon an enhanced linear correlation matching (ELCM) algorithm.  However, we noted that this approach depended on having data images available from multiple orbits and/or consecutive MISR blocks.  Since we do not have access to this full data set, reproducing this ELCM approach is not possible.

For that reason, we instead resort to more classic classification methods: QDA/LDA, logistic regression, and random forests.  First, we train baseline versions of these methods.   We note that random forests produces the best AUC values of the three baseline models, so we proceed to do additional parameter tuning and analysis for that model.

We decided before training these methods on a few approaches:

\begin{itemize}
\item \textbf{Three-fold cross validation. } Since labels of adjacent pixels in an image are not truly independent, we decided to perform three-fold cross validation: that is, we would train three different versions of each model, each time setting aside an entire image for testing purposes.\\
This is much preferred over randomly sampling points, since regions of ice and cloud often span many adjacent pixels, meaning that training on one of these pixels would enhance the test performance of the pixels nearby to it.
\item \textbf{Training only on positive- and negative- expert labels. } We do not train our model on data points whose expert classification was a 0 (unknown): we would only like our model to learn classification based on points whose true value is known.
\end{itemize}

\subsection{LDA/QDA}

<<fit-lda-qda, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2.5, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="ROC curve for LDA/QDA three-fold cross-validation.">>=

source("R/lda-qda.R")
source("R/explore.R")

path <- "data"
image1 <- read.table(paste0('data/', 'image1.txt'), header = F)
image2 <- read.table(paste0('data/', 'image2.txt'), header = F)
image3 <- read.table(paste0('data/', 'image3.txt'), header = F)

# Add informative column names.
collabs <- c('y','x','label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
names(image1) <- collabs
names(image2) <- collabs
names(image3) <- collabs

training.image <- list(image1,image2,image3)

lda.cv <- KFoldCV(training.image, method="lda")
qda.cv <- KFoldCV(training.image, method="qda")

lda.roc.info <- ggplot(lda.cv$roc) +
  geom_line(aes(x=FPR,y=TPR, group=fold, color=fold)) +
  scale_colour_discrete(name  ="Image Left Out") +
  ggtitle("LDA")

qda.roc.info <- ggplot(qda.cv$roc) +
  geom_line(aes(x=FPR,y=TPR, group=fold, color=fold)) +
  scale_colour_discrete(name  ="Image Left Out") +
  ggtitle("QDA")

grid_arrange_shared_legend(lda.roc.info,qda.roc.info,ncol =2 ,nrow = 1, position = "right")
@
Quadradic discriminant analysis(LDA) for classification aims to separate the feature space using lines(hyperplanes in higher dimenstion). It assumes that  each class $i$ has a Gaussian distribution with mean $\mu_i$ and covariance matrix $\Sigma_i$, that is 

$$
p(y = i | x) = \frac{\pi_i|2\pi\Sigma_i|^{-\frac{1}{2}}\exp[-\frac{1}{2}(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i)]}{\sum_k\pi_k|2\pi\Sigma_k|^{-\frac{1}{2}}\exp[-\frac{1}{2}(x-\mu_k)^T\Sigma_i^{-1}(x-\mu_k)]}
$$

Linear discriminant analysis(LDA) is a special case of QDA, which makes further assumption that the covariance matrix are the same across all classes. Our problem deals with binary data, and we can classify each point by selecting a probability threhold and assign a hard hyperline to determine whether a pixel belongs to clear or cloud. 

Figure ??? shows the ROC curves for the leave-one-image-out cross validation results from LDA and QDA models, in which QDA performs a little better than LDA in the first and second fold, which is in line with our exploratory analysis: the covariances for NDAI/CORR/SD may be difference between cloud and clear pixels. 

<<fit-qda-2, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2.5, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="AUC curve for LDA/QDA three-fold cross-validation">>=
auc.folds <- data.frame(LDA=lda.cv$auc,
                          QDA=qda.cv$auc,
                          Fold=1:3) %>% 
  melt(id="Fold", variable.name="Method",value.name="AUC") %>%
  ggplot() + 
  ggtitle("AUC scores") + 
  geom_point(aes(x=Fold, y=AUC, 
                 group=Method,colour=Method)) +
  scale_y_continuous(limits=c(0, 1)) +
  scale_x_discrete() +
  theme(aspect.ratio=1) +
  guides(size=FALSE)

auc.folds
@
Cross-validation for QDA and LDA tells that while they work really well in the first and second fold, finding that the AUC scores 
almost 1, it falls to perform as well in the third fold, where we regard image1 and image2 as training data and image3 as testing data. More specifically, the AUC scores for qda in the three folds are $0.960,0.971,0.888$ respectively, and for lda $0.951,0.954,0.895$.

<<fit-qda-3, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2.5, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Confusioin plots for qda">>=
qda.recon.loo <- ImageReconstruct(deconstructed=qda.cv$predictions, 
                                  nimages=3, xsplit=1, ysplit=1)
# Aesthetics
ggconf <- list(geom_point(aes(x=x, y=y, group=confusion, colour=confusion)),
               scale_colour_manual(values = c("True Positive" = "white",
                                              "False Positive" = "black",
                                              "True Negative" = "#FF9147", 
                                              "False Negative" = "#1e342b",
                                              "Unknown" = "#777777")),
               theme(legend.position="right", aspect.ratio=1))

qda1.loo.conf <- ggplot(qda.recon.loo$image1) + ggconf +
  ggtitle("Image 1") + no_axis
qda2.loo.conf <- ggplot(qda.recon.loo$image2) + ggconf +
  ggtitle("Image 2") + no_axis
qda3.loo.conf <- ggplot(qda.recon.loo$image3) + ggconf +
  ggtitle("Image 3") + no_axis

grid_arrange_shared_legend(qda1.loo.conf,qda2.loo.conf,qda2.loo.conf,ncol =3 ,nrow = 1, position = "right")

@
Figure ??? shows that confusion plot after we selected the threshold to be $0.11$, which is the average of the thresholds in the 
three-fold validation to make the point $(TRR, FPR)$ farest away from the line $y = x$. As the above plot shows, the classification suffers more from false positive and false negetive with some regions completely misclassified. Actually, those regions are hard to be classified from our bare eyes.  Calculation shows that the false positive rates of the three folds are $0.121,0.122,0.195$ and false negetive rates are $0.018,0.001,0.165$. The third fold cross-validation 
has such a high false negative rate because image3 has a small area of cloud and some parts of it on the upper left of the image are 
misclassified. However, the main problem is that the qda/lda classifier is hard to judge whether the "dark" regions in the images belong to cloud to clear, for example, the middle lower left region in image one, the lower right region in image two and lower middle and lower right regions on image three. All of that informs us of the irregularity of image three, for its large fraction of unknown area and complicated cloud and clear regions.

\subsection{Logistic regression}


\subsection{Random forests}

Random forests are an ensemble learning method that combine the prediction of multiple component decision trees in order to make a probabilistic determination of the class of an input.

Random forests are ``random" in a few different steps:
\begin{itemize}
\item Tree bagging. Each component tree randomly selects, via bootstrapping, a training subset from the total training set.
\item Feature bagging. At each candidate split of a component tree, a random subset of the features are used.
\end{itemize}

This is illustrated visually in the below picture.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.5\textwidth]{extra/rf_explanation.jpg}
\end{center}
\caption{Graphical introduction to random forests [2].}
\end{figure}

In our case, we fit random forest with 100 trees three times (using the three-fold cross validation approach described above).  The resulting ROC-curve is shown below, and reveals very strong performance, shown by the model's ability to achieve a high true positive rate with quite a low corresponding false negative rate.

<<fit-random-forest, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2.5, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="ROC curve for random forest three-fold cross-validation.">>=

source("R/random_forest.R")

# generated predictions for each of the three-fold CV data sets
prediction3 = trainAndEvaluateRF(train_data_12, image3_filtered, 100)
prediction2 = trainAndEvaluateRF(train_data_13, image2_filtered, 100)
prediction1 = trainAndEvaluateRF(train_data_23, image1_filtered, 100)

# calculate false positive and true positive rates into dataframe
fpr_tpr_rates_df <- data.frame(
  rbind(calculateROCValues(prediction3, 3),
      calculateROCValues(prediction2, 2),
      calculateROCValues(prediction1, 1)))
colnames(fpr_tpr_rates_df) <- c("fpr", "tpr", "image_num")

# plot results
ggplot(fpr_tpr_rates_df) +
  geom_line(aes(x=fpr, y=tpr, color=factor(image_num))) +
  labs(x = "False positive rate",
    y = "True positive rate",
    title = "Random Forest ROC curve: Leave-one-out CV") +
  scale_color_discrete(name = "Predictions on Image #",
                      labels = c("3 (AUC=0.885)",
                                 "2 (AUC=0.965)",
                                 "1 (AUC=0.956)")) +
  # clean up plot a little bit
  theme_bw() +
  # make the title and axes labels smaller
  theme(panel.border = element_blank(),
        legend.position = "right",
        legend.title = element_text(size=9),
        legend.text = element_text(size=9),
        plot.title = element_text(size=9),
        axis.title.x = element_text(size = 9),
        axis.title.y = element_text(size = 9))
@

\subsubsection{Parameter tuning}

\subsubsection{Systematic misclassification errors}

\subsubsection{Generalizability of the model}


\section{Concluding thoughts}

\section{References}

(1)  Shi, Tao, et al. "Daytime arctic cloud detection based on multi-angle satellite data with case studies." Journal of the American Statistical Association 103.482 (2008): 584-593.
(2) Random Forest Interactive Discussion with Example. \url{https://www.youtube.com/watch?v=ajTc5y3OqSQ}

\end{document}
